{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Simple SEDD Example"
      ],
      "metadata": {
        "id": "WSZk-NFCg6Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 0. Dataset Example\n",
        "raw_sentences = [\n",
        "    \"I love playing guitar\",\n",
        "    \"I enjoy cooking meals\",\n",
        "    \"I like painting landscapes\",\n",
        "    \"I adore writing poetry\",\n",
        "    \"I enjoy dancing daily\",\n",
        "    \"I love traveling abroad\",\n",
        "    \"I like reading books\",\n",
        "    \"I enjoy playing chess\",\n",
        "    \"I love coding projects\",\n",
        "    \"I prefer hiking mountains\",\n",
        "    \"I enjoy taking photos\",\n",
        "    \"I like swimming pools\",\n",
        "    \"I practice yoga daily\",\n",
        "    \"I enjoy video games\",\n",
        "    \"I love singing songs\",\n",
        "    \"I enjoy night driving\",\n",
        "    \"I like working out\",\n",
        "    \"I enjoy gardening flowers\",\n",
        "    \"I love baking cakes\",\n",
        "    \"I like watching sunsets\",\n",
        "    \"I enjoy learning languages\",\n",
        "    \"I love sketching nature\",\n",
        "    \"I like watching movies\",\n",
        "    \"I enjoy fishing trips\",\n",
        "    \"I love pet cuddles\",\n",
        "    \"I practice meditation daily\",\n",
        "    \"I enjoy riding bikes\",\n",
        "    \"I like solving puzzles\",\n",
        "    \"I enjoy board games\",\n",
        "    \"I prefer nature photography\",\n",
        "    \"I practice daily journaling\",\n",
        "    \"I love trying foods\",\n",
        "    \"I enjoy music playlists\",\n",
        "    \"I adore volunteering work\",\n",
        "    \"I love traveling solo\",\n",
        "    \"I enjoy watching documentaries\",\n",
        "    \"I practice skateboarding tricks\",\n",
        "    \"I love magic tricks\",\n",
        "    \"I prefer digital art\",\n",
        "    \"I enjoy playing piano\",\n",
        "    \"I adore home decorating\",\n",
        "    \"I love nature walks\",\n",
        "    \"I enjoy knitting sweaters\",\n",
        "    \"I love exploring history\",\n",
        "    \"I enjoy writing stories\",\n",
        "    \"I like editing videos\",\n",
        "    \"I enjoy studying cultures\",\n",
        "    \"I love skydiving adventures\",\n",
        "    \"I like building models\",\n",
        "    \"I enjoy restoring furniture\"\n",
        "]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "60ifrPlmhlv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Generate Dicitionary\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Converts a list of sentences (as strings) into a vocabulary-indexed format and ensures all sentences have the same length.\n",
        "\n",
        "    Args:\n",
        "    sentences (list of str): A list of sentences (each sentence is a string).\n",
        "\n",
        "    Returns:\n",
        "    indexed_sentences (list of list of int): Sentences converted to indices.\n",
        "    vocab (dict): Mapping of words to indices.\n",
        "    vocab_size (int): Number of unique words in the vocabulary.\n",
        "    \"\"\"\n",
        "    # Tokenize sentences into words\n",
        "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
        "\n",
        "    # Create vocabulary\n",
        "    unique_words = sorted(set(word for sentence in tokenized_sentences for word in sentence))  # Sort for consistent indexing\n",
        "    vocab = {word: idx for idx, word in enumerate(unique_words)}\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Convert sentences to indexed form\n",
        "    indexed_sentences = [[vocab[word] for word in sentence] for sentence in tokenized_sentences]\n",
        "\n",
        "    # Find the shortest sentence length\n",
        "    min_length = min(len(sentence) for sentence in indexed_sentences)\n",
        "\n",
        "    # Truncate sentences to match the shortest length\n",
        "    truncated_sentences = [sentence[:min_length] for sentence in indexed_sentences]\n",
        "\n",
        "    return truncated_sentences, vocab, vocab_size, min_length\n",
        "\n",
        "# Example usage\n",
        "# sentences = [\n",
        "#     \"hello world good morning\",\n",
        "#     \"world is beautiful\",\n",
        "#     \"good morning everyone today\"\n",
        "# ]\n",
        "sentences = raw_sentences\n",
        "indexed_sentences, vocab, vocab_size, sentence_length = preprocess_sentences(sentences)\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Indexed Sentences:\", indexed_sentences)\n",
        "print(\"Vocabulary Size:\", vocab_size)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vZPfG36VFOlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate Training Data\n",
        "import numpy as np\n",
        "\n",
        "def create_transition_matrix(vocab_size, t, T, sigma_min=0.0000001, sigma_max=0.000001, schedule=\"linear\"):\n",
        "    \"\"\"Creates a tridiagonal transition matrix Q_t with a noise scheduler.\"\"\"\n",
        "\n",
        "    # # Choose a noise scaling function\n",
        "    # if schedule == \"linear\":\n",
        "    #     sigma_t = sigma_min + t * (sigma_max - sigma_min) / T\n",
        "    # elif schedule == \"exponential\":\n",
        "    #     sigma_t = sigma_min * np.exp(2 * t / T)  # Adjust exponent as needed\n",
        "    # elif schedule == \"cosine\":\n",
        "    #     sigma_t = sigma_min + 0.5 * (sigma_max - sigma_min) * (1 - np.cos(np.pi * t / T))\n",
        "    # else:\n",
        "    #     raise ValueError(\"Invalid schedule type. Choose 'linear', 'exponential', or 'cosine'.\")\n",
        "    # sigma_t = 0.9\n",
        "    # Q_t = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "    # for i in range(vocab_size):\n",
        "    #     if i > 0:\n",
        "    #         Q_t[i, i - 1] = sigma_t  # Transition to previous index\n",
        "    #     if i < vocab_size - 1:\n",
        "    #         Q_t[i, i + 1] = sigma_t  # Transition to next index\n",
        "    #     Q_t[i, i] = -(Q_t[i, i - 1] if i > 0 else 0) - (Q_t[i, i + 1] if i < vocab_size - 1 else 0)\n",
        "\n",
        "    # alternative code\n",
        "    Q_t = np.full((vocab_size, vocab_size), 0.01)\n",
        "    np.fill_diagonal(Q_t, -np.sum(Q_t, axis=1))\n",
        "    # end of alternative code\n",
        "\n",
        "\n",
        "    return Q_t\n",
        "\n",
        "def forward_diffusion(p_t, Q_t, delta_t):\n",
        "    \"\"\"Performs one step of forward diffusion for each word separately.\"\"\"\n",
        "    return p_t + delta_t * np.dot(Q_t, p_t)\n",
        "\n",
        "def compute_pairwise_ratios(p_t):\n",
        "    \"\"\"Computes the probability ratios p_t(y) / p_t(x) for all pairs (x, y) in each row.\"\"\"\n",
        "    vocab_size = p_t.shape[1]\n",
        "    ratios = np.zeros((p_t.shape[0], vocab_size, vocab_size))\n",
        "\n",
        "    for x in range(p_t.shape[0]):  # Word level\n",
        "        for y in range(vocab_size):  # The probability level\n",
        "            for z in range(vocab_size):  # Each other probability\n",
        "                if p_t[x, y] > 0:  # Only compute ratios for non-zero elements\n",
        "                    ratios[x, y, z] = p_t[x, z] / p_t[x, y]  # Divide element by the diagonal element in the same row\n",
        "\n",
        "    return ratios\n",
        "\n",
        "def sample_new_indices(p_t1):\n",
        "    \"\"\"Samples new word indices for each word separately using its own probability distribution.\"\"\"\n",
        "    new_sentence = []\n",
        "    for word_dist in p_t1:\n",
        "        new_word = np.random.choice(len(word_dist), p=word_dist / np.sum(word_dist))  # Normalize to ensure valid probabilities\n",
        "        new_sentence.append(new_word)\n",
        "    return new_sentence\n",
        "\n",
        "def generate_training_data(sentences, vocab_size, delta_t, num_steps, schedule=\"linear\"):\n",
        "    \"\"\"Generates training data with a noise scheduler applied to Q_t.\"\"\"\n",
        "    X_train = []\n",
        "    Y_train = []\n",
        "    Z_train = []\n",
        "\n",
        "    first_sentence = sentences[0]  # Track only the first sentence\n",
        "\n",
        "    # print(f\"Step 0 (Initial Sentence): {first_sentence}\")  # Print initial state\n",
        "\n",
        "    for sentence in sentences:\n",
        "        p_t = np.full((len(sentence), vocab_size), 1e-6)\n",
        "        for i, word in enumerate(sentence):\n",
        "            p_t[i, word] = 1  # Start with a delta distribution\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            Q_t = create_transition_matrix(vocab_size, step, num_steps, schedule=schedule)\n",
        "            pairwise_ratios = compute_pairwise_ratios(p_t)\n",
        "            p_t1 = np.array([forward_diffusion(p_t[i], Q_t, delta_t) for i in range(len(sentence))])\n",
        "            new_sentence = sample_new_indices(p_t1)\n",
        "\n",
        "            X_train.append(new_sentence)\n",
        "            Y_train.append(pairwise_ratios)\n",
        "            Z_train.append(p_t)\n",
        "            print(sentence)\n",
        "            # if sentence is first_sentence:\n",
        "            #     print(f\"Step {step + 1}: {new_sentence}\")  # Print progression of the first sentence\n",
        "            #     print(\"times\")\n",
        "            sentence = new_sentence\n",
        "            p_t = p_t1  # Update probabilities for the next step\n",
        "\n",
        "    return X_train, Y_train, Z_train, Q_t\n",
        "\n",
        "# Example usage\n",
        "delta_t = 0.1\n",
        "sentences = indexed_sentences  # Example word indices\n",
        "X_train, Y_train, Z_train, Q_t = generate_training_data(sentences, vocab_size, delta_t, num_steps=10)\n",
        "\n",
        "# Print extracted training data\n",
        "# print(\"X_train (word indices at t):\", X_train)\n",
        "# print(\"Y_train (pairwise ratios from t-1):\", Y_train)\n",
        "# print(\"Z_train (probabilities from t-1):\", Z_train)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PcfeQOfl4i_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Train a model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, Reshape\n",
        "\n",
        "\n",
        "# Normalize Y_train\n",
        "Y_min = np.min(Y_train)\n",
        "Y_max = np.max(Y_train)\n",
        "\n",
        "\n",
        "# X and Y train from actual data\n",
        "# Convert lists to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "Y_train_normalized = (Y_train - Y_min) / (Y_max - Y_min)  # Normalize to [0,1]\n",
        "\n",
        "# Get output shape\n",
        "output_shape = Y_train.shape[1:]  # Extract shape from generated Y_train\n",
        "output_units = np.prod(output_shape)  # Total number of output elements\n",
        "\n",
        "\n",
        "\n",
        "# end of X and Y train from actual data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Define the model\n",
        "model = Sequential([\n",
        "    Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer\n",
        "    Dense(256, activation=\"relu\"),  # Second hidden layer\n",
        "    Dense(output_units, activation=\"linear\"),  # Output layer (flattened version of (2,3,3))\n",
        "    Reshape(output_shape)  # Reshape to (2,3,3)\n",
        "])\n",
        "\n",
        "# Define the model\n",
        "# model = Sequential([\n",
        "#     Dense(512, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer\n",
        "#     Dense(512, activation=\"relu\"),  # Second hidden layer\n",
        "#     Dense(256, activation=\"relu\"),  # Third hidden layer\n",
        "#     Dense(256, activation=\"relu\"),  # Fourth hidden layer\n",
        "#     Dense(128, activation=\"relu\"),  # Fifth hidden layer\n",
        "#     Dense(128, activation=\"relu\"),  # Sixth hidden layer\n",
        "#     Dense(output_units, activation=\"linear\"),  # Output layer (flattened version of (2,3,3))\n",
        "#     Reshape(output_shape)  # Reshape to (2,3,3)\n",
        "# ])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train_normalized, epochs=20, batch_size=16, verbose=1)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_train[1:3])  # Predict for the first 5 samples\n",
        "print(\"Predicted Y_train shape:\", predictions.shape)  # Should match (5, 2, 3, 3)\n"
      ],
      "metadata": {
        "id": "puHQ6bobJMCv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Backward diffusion using model ratios\n",
        "\n",
        "#@title 2. The Revesed Matrix Construction (forward and backward diffusion demo)\n",
        "\n",
        "# Y_train[0]   #==> model output (ratios of a sequence)\n",
        "# This function takes in the ratios of a word in the sequence. (ratios = Y_train[0][i])\n",
        "def estimate_Qt_inv_from_ratios(Q_t, ratios):\n",
        "  vocab_size = len(Q_t)\n",
        "  Q_inv = np.zeros_like(Q_t, dtype=np.float64)\n",
        "\n",
        "  for x in range(vocab_size):\n",
        "      for y in range(vocab_size):\n",
        "          if x != y:\n",
        "              # Use stored pairwise ratio instead of computing from p_t\n",
        "              Q_inv[y, x] = ratios[x, y] * Q_t[x, y]     #ratios[x, y] means p(y)/p(x) where the index of the word is the same as its index in the ratios matrix\n",
        "\n",
        "  for x in range(vocab_size):\n",
        "      Q_inv[x, x] = -np.sum(Q_inv[:, x][Q_inv[:, x] != Q_inv[x, x]])\n",
        "\n",
        "  return Q_inv\n",
        "\n",
        "def backward_diffusion(P_new, Q_t, delta_t, ratios):   # on the word level\n",
        "    \"\"\"Performs one step of backward diffusion using estimated Qt_inv.\"\"\"\n",
        "    I = np.eye(len(Q_t))  # Identity matrix\n",
        "    Q_t_bar = estimate_Qt_inv_from_ratios(Q_t, ratios)  # Compute reversed transitio matrix\n",
        "\n",
        "    # Compute P_old using the formula from the image: P_old = P_new * (I - Δt * Q_t_bar)^(-1)\n",
        "    P_old = np.linalg.inv(I - delta_t * Q_t_bar) @ P_new\n",
        "\n",
        "    return P_old\n",
        "\n",
        "\n",
        "# the following function takes in p_t of a sequence and returns p_t_reconvered (at t-1)\n",
        "def sentence_backward_diffusion(p_t, sen_ratios, Q_t, delta_t):\n",
        "  sequence_len = p_t.shape[0]  #each word is a row\n",
        "  #print(\"sequence len is\", sequence_len)\n",
        "  p_t_recovered = []\n",
        "  for i in range(sequence_len):    # for each word\n",
        "      P_new = p_t[i] # probabilitys of a word (each row is a word)\n",
        "      sen_ratios = np.squeeze(sen_ratios)\n",
        "      ratios = sen_ratios[i]   # ratios of a word\n",
        "\n",
        "      P_recovered = backward_diffusion(P_new, Q_t, delta_t, ratios)\n",
        "      p_t_recovered.append(P_recovered)\n",
        "\n",
        "  return p_t_recovered\n",
        "\n",
        "# Initialize random probability distribution (assuming vocab_size = 3 for shape consistency)\n",
        "def initialize_random_distribution(sentence_length, vocab_size):\n",
        "    \"\"\"Creates a random probability distribution for a given sentence length.\"\"\"\n",
        "    p_t = np.random.rand(sentence_length, vocab_size)\n",
        "    p_t /= p_t.sum(axis=1, keepdims=True)  # Normalize\n",
        "    return p_t\n",
        "\n",
        "# p_t_original = np.array([[1.e-06, 1.e+00, 1.e-06],\n",
        "#                           [1.e-06, 1.e-06, 1.e+00]])  # Added missing bracket\n",
        "\n",
        "# p_t1 = np.array([[9.0000910e-02, 8.2000018e-01, 9.0000910e-02],\n",
        "#                  [1.0000000e-06, 9.0000910e-02, 9.1000009e-01]])  # No extra bracket\n",
        "\n",
        "# p_t_original = np.array([[9.0000910e-02, 8.2000018e-01, 9.0000910e-02],\n",
        "#                          [1.0000000e-06, 9.0000910e-02, 9.1000009e-01]])  # Correct\n",
        "\n",
        "# p_t1 = np.array([[0.15570084, 0.68860031, 0.15570084],\n",
        "#                  [0.00810099, 0.15570084, 0.83620016]])  # Removed extra bracket\n",
        "\n",
        "# p_t_original = Z_train[1]\n",
        "# p_t1 = Z_train[2]\n",
        "\n",
        "# sample = X_train[0].reshape(1, -1)  # Reshape to (1, input_dim) to keep batch dimension\n",
        "# # print(\"sample is\", sample)\n",
        "# predictions_normalized = model.predict(sample)\n",
        "# predictions = predictions_normalized * (Y_max - Y_min) + Y_min\n",
        "\n",
        "# # print(\"Prediction\", predictions)  # Expected output: (1, 2, 3, 3)\n",
        "\n",
        "# p_t_recovered = sentence_backward_diffusion(p_t1, predictions, Q_t, delta_t)\n",
        "\n",
        "\n",
        "# print(\"p_t + 1 is\", p_t1)\n",
        "# print(\"p_t original is\", p_t_original)\n",
        "# print(\"p_t is\", p_t_recovered)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "msarWQrZ0_CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5 - SEED\n",
        "# SEED = 100\n",
        "# p_t = Z_train[SEED]\n",
        "p_t = initialize_random_distribution(sentence_length, vocab_size)\n",
        "\n",
        "# List to store all samples\n",
        "all_samples = []\n"
      ],
      "metadata": {
        "id": "drawrxG_1Oho",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6 - NEXT STEP\n",
        "\n",
        "#@title 5. Mutli-Step Generation (saving all the steps)\n",
        "import numpy as np\n",
        "\n",
        "# Define function to sample new indices\n",
        "def sample_new_indices(p_t1):\n",
        "    \"\"\"Samples new word indices for each word separately using its own probability distribution.\"\"\"\n",
        "    new_sentence = []\n",
        "    for word_dist in p_t1:\n",
        "        word_dist[word_dist < 0] = 0  # Fix negative values\n",
        "        new_word = np.random.choice(len(word_dist), p=word_dist/np.sum(word_dist))  # Normalize\n",
        "        new_sentence.append(new_word)\n",
        "    return np.array(new_sentence)\n",
        "\n",
        "\n",
        "def indices_to_sentence(indexed_sentences, vocab):\n",
        "    \"\"\"\n",
        "    Converts indexed sentences back to human-readable sentences using the vocabulary.\n",
        "\n",
        "    Args:\n",
        "    indexed_sentences (list of list of int): List of sentences represented as word indices.\n",
        "    vocab (dict): Dictionary mapping words to indices.\n",
        "\n",
        "    Returns:\n",
        "    sentences (list of str): List of reconstructed sentences.\n",
        "    \"\"\"\n",
        "    # Reverse the vocabulary (index → word)\n",
        "    index_to_word = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "    # Convert each indexed sentence back to words\n",
        "    sentences = [\" \".join(index_to_word[idx] for idx in sentence) for sentence in indexed_sentences]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate new sample based on current probability distribution\n",
        "sample = sample_new_indices(p_t).reshape(1, -1)\n",
        "all_samples.append(sample)  # Save the sample\n",
        "# print(f\"Step {step+1} - Sample: {sample}\")\n",
        "\n",
        "# Predict new probabilities using the model\n",
        "predictions_normalized = model.predict(sample)\n",
        "predictions = predictions_normalized * (Y_max - Y_min) + Y_min\n",
        "\n",
        "# Update probability distribution\n",
        "p_t = sentence_backward_diffusion(p_t, predictions, Q_t, delta_t)\n",
        "p_t = np.array(p_t)\n",
        "\n",
        "\n",
        "Final_Sentence = indices_to_sentence(sample, vocab)\n",
        "print(Final_Sentence)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6uIK2at81bHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other ---"
      ],
      "metadata": {
        "id": "_UV2-RoGKisc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Forward and Backward diffusion (Final Code)(16-3-2025)(demo)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def forward_diffusion(P_old, Q_t, delta_t):\n",
        "    \"\"\"Performs one step of forward diffusion.\"\"\"\n",
        "    return P_old + delta_t * np.dot(Q_t, P_old)\n",
        "\n",
        "def estimate_Qt_inv(Q_t, p_t):\n",
        "    \"\"\"Estimate the reversed transition matrix Q_inv using probability ratio estimation.\"\"\"\n",
        "    Q_inv = np.zeros_like(Q_t, dtype=np.float64)\n",
        "\n",
        "    for x in range(len(Q_t)):\n",
        "        for y in range(len(Q_t)):\n",
        "            if x != y:\n",
        "                Q_inv[y, x] = (p_t[y] / p_t[x]) * Q_t[x, y]\n",
        "\n",
        "    for x in range(len(Q_t)):\n",
        "        Q_inv[x, x] = -np.sum(Q_inv[:, x][Q_inv[:, x] != Q_inv[x, x]])\n",
        "\n",
        "    print(\"Q inverse is\")\n",
        "    print(Q_inv)\n",
        "\n",
        "    return Q_inv\n",
        "\n",
        "def backward_diffusion(P_new, Q_t, delta_t, p_t):\n",
        "    \"\"\"Performs one step of backward diffusion using estimated Qt_inv.\"\"\"\n",
        "    I = np.eye(len(Q_t))  # Identity matrix\n",
        "    Q_t_bar = estimate_Qt_inv(Q_t, p_t)  # Compute reversed transitio matrix\n",
        "\n",
        "    # Compute P_old using the formula from the image: P_old = P_new * (I - Δt * Q_t_bar)^(-1)\n",
        "    P_old = np.linalg.inv(I - delta_t * Q_t_bar) @ P_new\n",
        "\n",
        "    return P_old\n",
        "\n",
        "\n",
        "Q_t = np.array([[-2, 1, 1],\n",
        "                [1, -2, 1],\n",
        "                [1, 1, -2]])\n",
        "\n",
        "\n",
        "# Initial probability distribution\n",
        "# P_old = np.array([0.7, 0.2, 0.1])\n",
        "P_old = np.array([0.5, 0.3, 0.2])\n",
        "\n",
        "# Time step\n",
        "delta_t = 0.1\n",
        "\n",
        "# Forward diffusion\n",
        "P_new = forward_diffusion(P_old, Q_t, delta_t)\n",
        "print(\"P_new (after forward diffusion):\", P_new)\n",
        "print(\"Difference between P_new and P_old:\", np.linalg.norm(P_new - P_old))\n",
        "\n",
        "# Backward diffusion\n",
        "P_recovered = backward_diffusion(P_new, Q_t, delta_t, P_old)\n",
        "print(\"P_old (after backward diffusion, should be close to original):\", P_recovered)\n",
        "print(\"Difference between P_recovered and P_old:\", np.linalg.norm(P_recovered - P_old))\n"
      ],
      "metadata": {
        "id": "AIemkzxILGUL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inferrence (demo)\n",
        "sample = X_train[0].reshape(1, -1)  # Reshape to (1, input_dim) to keep batch dimension\n",
        "predictions_normalized = model.predict(sample)\n",
        "predictions = predictions_normalized * (Y_max - Y_min) + Y_min\n",
        "\n",
        "print(\"Prediction\", predictions)  # Expected output: (1, 2, 3, 3)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dDzo5Pyys-b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title The Revesed Matrix Construction (forward and backward diffusion) (demo)\n",
        "\n",
        "# Y_train[0]   #==> model output (ratios of a sequence)\n",
        "# This function takes in the ratios of a word in the sequence. (ratios = Y_train[0][i])\n",
        "def estimate_Qt_inv_from_ratios(Q_t, ratios):\n",
        "  vocab_size = len(Q_t)\n",
        "  Q_inv = np.zeros_like(Q_t, dtype=np.float64)\n",
        "\n",
        "  for x in range(vocab_size):\n",
        "      for y in range(vocab_size):\n",
        "          if x != y:\n",
        "              # Use stored pairwise ratio instead of computing from p_t\n",
        "              Q_inv[y, x] = ratios[x, y] * Q_t[x, y]     #ratios[x, y] means p(y)/p(x) where the index of the word is the same as its index in the ratios matrix\n",
        "\n",
        "  for x in range(vocab_size):\n",
        "      Q_inv[x, x] = -np.sum(Q_inv[:, x][Q_inv[:, x] != Q_inv[x, x]])\n",
        "\n",
        "  return Q_inv\n",
        "\n",
        "def backward_diffusion(P_new, Q_t, delta_t, ratios):   # on the word level\n",
        "    \"\"\"Performs one step of backward diffusion using estimated Qt_inv.\"\"\"\n",
        "    I = np.eye(len(Q_t))  # Identity matrix\n",
        "    Q_t_bar = estimate_Qt_inv_from_ratios(Q_t, ratios)  # Compute reversed transitio matrix\n",
        "\n",
        "    # Compute P_old using the formula from the image: P_old = P_new * (I - Δt * Q_t_bar)^(-1)\n",
        "    P_old = np.linalg.inv(I - delta_t * Q_t_bar) @ P_new\n",
        "\n",
        "    return P_old\n",
        "\n",
        "\n",
        "# the following function takes in p_t of a sequence and returns p_t_reconvered (at t-1)\n",
        "def sentence_backward_diffusion(p_t, Y_train, Q_t, delta_t):\n",
        "  sequence_len = p_t.shape[0]  #each word is a row\n",
        "  print(\"sequence len is\", sequence_len)\n",
        "  p_t_recovered = []\n",
        "  for i in range(sequence_len):    # for each word\n",
        "      print(\"i is\")\n",
        "      print(i)\n",
        "      P_new = p_t[i] # probabilitys of a word (each row is a word)\n",
        "\n",
        "\n",
        "      model_output = Y_train[1]\n",
        "\n",
        "      ratios = Y_train[1][i]   # ratios of a word\n",
        "      print(\"input is \")\n",
        "      print(Y_train[1])\n",
        "      P_recovered = backward_diffusion(P_new, Q_t, delta_t, ratios)\n",
        "      p_t_recovered.append(P_recovered)\n",
        "\n",
        "  return p_t_recovered\n",
        "\n",
        "# p_t_original = np.array([[1.e-06, 1.e+00, 1.e-06],\n",
        "#                           [1.e-06, 1.e-06, 1.e+00]])  # Added missing bracket\n",
        "\n",
        "# p_t1 = np.array([[9.0000910e-02, 8.2000018e-01, 9.0000910e-02],\n",
        "#                  [1.0000000e-06, 9.0000910e-02, 9.1000009e-01]])  # No extra bracket\n",
        "\n",
        "# p_t_original = np.array([[9.0000910e-02, 8.2000018e-01, 9.0000910e-02],\n",
        "#                          [1.0000000e-06, 9.0000910e-02, 9.1000009e-01]])  # Correct\n",
        "\n",
        "# p_t1 = np.array([[0.15570084, 0.68860031, 0.15570084],\n",
        "#                  [0.00810099, 0.15570084, 0.83620016]])  # Removed extra bracket\n",
        "\n",
        "p_t_original = Z_train[1]\n",
        "p_t1 = Z_train[2]\n",
        "\n",
        "p_t_recovered = sentence_backward_diffusion(p_t1, Y_train, Q_t, delta_t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"p_t + 1 is\", p_t1)\n",
        "print(\"p_t original is\", p_t_original)\n",
        "print(\"p_t is\", p_t_recovered)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CZdCMb83vPGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Generate Training Data (variable noise schadual)(option 2)\n",
        "import numpy as np\n",
        "\n",
        "def create_transition_matrix(vocab_size, t, T, sigma_min=0.01, sigma_max=0.1, schedule=\"linear\"):\n",
        "    \"\"\"Creates a tridiagonal transition matrix Q_t with a noise scheduler.\"\"\"\n",
        "\n",
        "    # Choose a noise scaling function\n",
        "    if schedule == \"linear\":\n",
        "        sigma_t = sigma_min + t * (sigma_max - sigma_min) / T\n",
        "    elif schedule == \"exponential\":\n",
        "        sigma_t = sigma_min * np.exp(2 * t / T)  # Adjust exponent as needed\n",
        "    elif schedule == \"cosine\":\n",
        "        sigma_t = sigma_min + 0.5 * (sigma_max - sigma_min) * (1 - np.cos(np.pi * t / T))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid schedule type. Choose 'linear', 'exponential', or 'cosine'.\")\n",
        "\n",
        "    Q_t = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        if i > 0:\n",
        "            Q_t[i, i - 1] = sigma_t  # Transition to previous index\n",
        "        if i < vocab_size - 1:\n",
        "            Q_t[i, i + 1] = sigma_t  # Transition to next index\n",
        "        Q_t[i, i] = -(Q_t[i, i - 1] if i > 0 else 0) - (Q_t[i, i + 1] if i < vocab_size - 1 else 0)\n",
        "\n",
        "    return Q_t\n",
        "\n",
        "def forward_diffusion(p_t, Q_t, delta_t):\n",
        "    \"\"\"Performs one step of forward diffusion for each word separately.\"\"\"\n",
        "    return p_t + delta_t * np.dot(Q_t, p_t)\n",
        "\n",
        "\n",
        "def compute_pairwise_ratios(p_t):\n",
        "    \"\"\"Computes the probability ratios p_t(y) / p_t(x) for all pairs (x, y) in each row.\"\"\"\n",
        "    print(p_t.shape)\n",
        "    vocab_size = p_t.shape[1]\n",
        "    print(vocab_size)\n",
        "    ratios = np.zeros((p_t.shape[0], vocab_size, vocab_size))\n",
        "    print(\"ratios shape is\")\n",
        "    print(ratios.shape)\n",
        "    for x in range(p_t.shape[0]): # word level\n",
        "        for y in range(vocab_size): # the probablities level\n",
        "            for z in range(vocab_size): # each other probabilty\n",
        "              if p_t[x, y] > 0:  # Only compute ratios for non-zero elements\n",
        "                  ratios[x, y, z] = p_t[x, z] / p_t[x, y]  # Divide element by the diagonal element in the same row\n",
        "\n",
        "    return ratios\n",
        "\n",
        "def sample_new_indices(p_t1):\n",
        "    \"\"\"Samples new word indices for each word separately using its own probability distribution.\"\"\"\n",
        "    new_sentence = []\n",
        "    for word_dist in p_t1:\n",
        "        new_word = np.random.choice(len(word_dist), p=word_dist/np.sum(word_dist))  # Normalize to ensure valid probabilities\n",
        "        new_sentence.append(new_word)\n",
        "    return new_sentence\n",
        "\n",
        "def generate_training_data(sentences, vocab_size, delta_t, num_steps, schedule=\"exponential\"):\n",
        "    \"\"\"Generates training data with a noise scheduler applied to Q_t.\"\"\"\n",
        "    X_train = []\n",
        "    Y_train = []\n",
        "    Z_train = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        p_t = np.full((len(sentence), vocab_size), 1e-2)\n",
        "        for i, word in enumerate(sentence):\n",
        "            p_t[i, word] = 1  # Start with a delta distribution\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            Q_t = create_transition_matrix(vocab_size, step, num_steps, schedule=schedule)\n",
        "            pairwise_ratios = compute_pairwise_ratios(p_t)\n",
        "            p_t1 = np.array([forward_diffusion(p_t[i], Q_t, delta_t) for i in range(len(sentence))])\n",
        "            new_sentence = sample_new_indices(p_t1)\n",
        "\n",
        "            X_train.append(new_sentence)\n",
        "            Y_train.append(pairwise_ratios)\n",
        "            Z_train.append(p_t)\n",
        "\n",
        "            sentence = new_sentence\n",
        "            p_t = p_t1  # Update probabilities for next step\n",
        "\n",
        "    return X_train, Y_train, Z_train, Q_t\n",
        "\n",
        "# Example usage\n",
        "delta_t = 0.1\n",
        "sentences = indexed_sentences  # Example word indices\n",
        "X_train, Y_train, Z_train, Q_t = generate_training_data(sentences, vocab_size, delta_t, num_steps=10)\n",
        "\n",
        "# Print extracted training data\n",
        "print(\"X_train (word indices at t):\", X_train)\n",
        "print(\"Y_train (pairwise ratios from t-1):\", Y_train)\n",
        "print(\"Z_train (probabilities from t-1):\", Z_train)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mnL60V0S0dPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Generate Training Data (constant noise schadual)(option 1)\n",
        "import numpy as np\n",
        "\n",
        "def create_transition_matrix(vocab_size, rate=0.00001):\n",
        "    \"\"\"Creates a tridiagonal transition matrix Q_t for discrete diffusion.\"\"\"\n",
        "    Q_t = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        if i > 0:\n",
        "            Q_t[i, i - 1] = rate  # Transition to previous index\n",
        "        if i < vocab_size - 1:\n",
        "            Q_t[i, i + 1] = rate  # Transition to next index\n",
        "        Q_t[i, i] = -(Q_t[i, i - 1] if i > 0 else 0) - (Q_t[i, i + 1] if i < vocab_size - 1 else 0)\n",
        "\n",
        "    return Q_t\n",
        "\n",
        "def forward_diffusion(p_t, Q_t, delta_t):\n",
        "    \"\"\"Performs one step of forward diffusion for each word separately.\"\"\"\n",
        "    return p_t + delta_t * np.dot(Q_t, p_t)\n",
        "\n",
        "\n",
        "def compute_pairwise_ratios(p_t):\n",
        "    \"\"\"Computes the probability ratios p_t(y) / p_t(x) for all pairs (x, y) in each row.\"\"\"\n",
        "    # print(p_t.shape)\n",
        "    vocab_size = p_t.shape[1]\n",
        "    # print(vocab_size)\n",
        "    ratios = np.zeros((p_t.shape[0], vocab_size, vocab_size))\n",
        "    # print(\"ratios shape is\")\n",
        "    # print(ratios.shape)\n",
        "    for x in range(p_t.shape[0]): # word level\n",
        "        for y in range(vocab_size): # the probablities level\n",
        "            for z in range(vocab_size): # each other probabilty\n",
        "              if p_t[x, y] > 0:  # Only compute ratios for non-zero elements\n",
        "                  ratios[x, y, z] = p_t[x, z] / p_t[x, y]  # Divide element by the diagonal element in the same row\n",
        "\n",
        "    return ratios\n",
        "\n",
        "def sample_new_indices(p_t1):\n",
        "    \"\"\"Samples new word indices for each word separately using its own probability distribution.\"\"\"\n",
        "    new_sentence = []\n",
        "    for word_dist in p_t1:\n",
        "        new_word = np.random.choice(len(word_dist), p=word_dist/np.sum(word_dist))  # Normalize to ensure valid probabilities\n",
        "        new_sentence.append(new_word)\n",
        "    return new_sentence\n",
        "\n",
        "def generate_training_data(sentences, vocab_size, delta_t, num_steps):\n",
        "    \"\"\"Generates X_train and Y_train for model training.\"\"\"\n",
        "    Q_t = create_transition_matrix(vocab_size)\n",
        "    print(\"Q_t is\")\n",
        "    print(Q_t)\n",
        "    X_train = []  # Stores word indices at time t\n",
        "    Y_train = []  # Stores pairwise ratios from time t-1\n",
        "    Z_train = []  # Stores the probabilities from time t-1\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # p_t = np.zeros((len(sentence), vocab_size))  # Each word has its own probability distribution\n",
        "        p_t = np.full((len(sentence), vocab_size), 1e-2)  # Initialize with 10^-6\n",
        "        # print(\"here\")\n",
        "        # print(p_t.shape)\n",
        "        for i, word in enumerate(sentence):\n",
        "            p_t[i, word] = 1  # Start with a delta distribution for each word\n",
        "        # print(\"initialization\")\n",
        "        # print(p_t)\n",
        "        # prev_ratios = None  # Store ratios from t-1\n",
        "        for step in range(num_steps):\n",
        "            pairwise_ratios = compute_pairwise_ratios(p_t)  # Compute ratios for each word separately\n",
        "            p_t1 = np.array([forward_diffusion(p_t[i], Q_t, delta_t) for i in range(len(sentence))])  # Apply diffusion separately\n",
        "            new_sentence = sample_new_indices(p_t1)  # Sample new indices for each word\n",
        "\n",
        "            # if prev_ratios is not None:\n",
        "            X_train.append(new_sentence)  # Store word indices at t\n",
        "            Y_train.append(pairwise_ratios)  # Store ratios from t-1\n",
        "            Z_train.append(p_t)\n",
        "\n",
        "            # Print (X1, Y1) pairs\n",
        "            # print(f\"Time Step {step}:\")\n",
        "            # for i in range(len(sentence)):\n",
        "            #     print(f\"Word {sentence[i]} transitioned to {new_sentence[i]}\")\n",
        "            print(sentence)\n",
        "            sentence = new_sentence  # Update sentence for next step\n",
        "            p_t = p_t1  # Update for next step\n",
        "            # prev_ratios = pairwise_ratios  # Store current ratios for next step\n",
        "\n",
        "    return X_train, Y_train, Z_train, Q_t\n",
        "\n",
        "# Example usage\n",
        "delta_t = 0.000001\n",
        "sentences = indexed_sentences  # Example word indices\n",
        "X_train, Y_train, Z_train, Q_t = generate_training_data(sentences, vocab_size, delta_t, num_steps=10)\n",
        "\n",
        "# Print extracted training data\n",
        "# print(\"X_train (word indices at t):\", X_train)\n",
        "# print(\"Y_train (pairwise ratios from t-1):\", Y_train)\n",
        "# print(\"Z_train (probabilities from t-1):\", Z_train)\n"
      ],
      "metadata": {
        "id": "Gr7PIQb-ySNO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}